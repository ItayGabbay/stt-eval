{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:27:27.078528Z",
     "start_time": "2023-10-14T12:27:23.526511Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "import librosa\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:22:41.040194Z",
     "start_time": "2023-10-14T12:22:40.624971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Arabic\", task=\"transcribe\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Arabic\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:22:45.274164Z",
     "start_time": "2023-10-14T12:22:41.042176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error openning ../dataset/test-txt/00a6d967-6af0-4018-bb31-6633be63dc1d.txt\n"
     ]
    }
   ],
   "source": [
    "inputs_path = \"../dataset/test\"\n",
    "labels_path = \"../dataset/test-txt\"\n",
    "dataset = []\n",
    "errors = 0\n",
    "for audio_file in os.listdir(inputs_path):\n",
    "    audio_data, sample_rate = librosa.load(os.path.join(inputs_path, audio_file))\n",
    "    label_file = os.path.join(labels_path, audio_file.split('.')[0] + \".txt\")\n",
    "    try:\n",
    "        with open(label_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            text = f.read().strip()\n",
    "\n",
    "        dataset.append(\n",
    "            {\"audio_data\": audio_data, \"sample_rate\": sample_rate, \"sentence\": text}\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Error openning {label_file}\")\n",
    "        errors += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:22:45.284470Z",
     "start_time": "2023-10-14T12:22:45.277959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:22:57.215784Z",
     "start_time": "2023-10-14T12:22:45.286324Z"
    }
   },
   "outputs": [],
   "source": [
    "# resample data - to remove once data is already in 16k sr\n",
    "for record in dataset:\n",
    "    data = record['audio_data']\n",
    "    origin_sr = record['sample_rate']\n",
    "    expected_sr = 16000\n",
    "    data_resampled = resample(data, int(len(data) * expected_sr / origin_sr), axis=0)\n",
    "    record['sample_rate'] = expected_sr\n",
    "    record['audio_data'] = data_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:22:57.222676Z",
     "start_time": "2023-10-14T12:22:57.218190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_data': array([-0.03481539,  0.02195274,  0.04504649, ...,  0.17630008,\n",
       "         0.14643314,  0.03439737], dtype=float32),\n",
       " 'sample_rate': 16000,\n",
       " 'sentence': 'جيد استلمت لك افضل التحيات'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:22:57.232942Z",
     "start_time": "2023-10-14T12:22:57.224546Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_record(record):\n",
    "    # compute log-Mel input features from input audio array \n",
    "    record[\"input_features\"] = feature_extractor(record[\"audio_data\"], sampling_rate=record[\"sample_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    record[\"labels\"] = tokenizer(record[\"sentence\"]).input_ids\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:09.155212Z",
     "start_time": "2023-10-14T12:22:57.234874Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1348/1348 [01:11<00:00, 18.75it/s]\n"
     ]
    }
   ],
   "source": [
    "prepared_records = []\n",
    "for record in tqdm(dataset):\n",
    "    prepared_records.append(prepare_record(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:09.170131Z",
     "start_time": "2023-10-14T12:24:09.161364Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:09.186590Z",
     "start_time": "2023-10-14T12:24:09.179440Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:09.406398Z",
     "start_time": "2023-10-14T12:24:09.188422Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:09.412330Z",
     "start_time": "2023-10-14T12:24:09.408467Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:12.816158Z",
     "start_time": "2023-10-14T12:24:09.414121Z"
    }
   },
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:12.821296Z",
     "start_time": "2023-10-14T12:24:12.818411Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:13.848504Z",
     "start_time": "2023-10-14T12:24:12.823129Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-ar\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T12:24:15.495489Z",
     "start_time": "2023-10-14T12:24:13.850547Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=prepared_records[:1000],\n",
    "    eval_dataset=prepared_records[1000:],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-14T12:18:33.286Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eyesondatadisk/anaconda3/envs/speech/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:speech]",
   "language": "python",
   "name": "conda-env-speech-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
